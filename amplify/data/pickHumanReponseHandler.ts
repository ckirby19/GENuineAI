import { type Schema } from "./resource";
import {
  BedrockRuntimeClient,
  InvokeModelCommand,
  InvokeModelCommandInput,
} from "@aws-sdk/client-bedrock-runtime";

export const handler: Schema["PickHumanResponse"]["functionHandler"] = async (
  event,
  context
) => {
    if (!event.arguments.originalPrompt) {
        throw new Error("Missing original prompt");
    }
    if (!event.arguments.answers) {
        throw new Error("Missing answers");
    }
    if (!event.arguments.model) {
        throw new Error("Missing model");
    }
    console.log(
        `Pick Human Response Handler Started - for prompt ${event.arguments.originalPrompt} receieved answers to choose from:
        ${event.arguments.answers}`);
    const AWS_REGION = process.env.AWS_REGION || "eu-west-2";

    const client = new BedrockRuntimeClient({
        region: AWS_REGION
    });
    
    if (event.arguments.model == "mistral.mistral-7b-instruct-v0:2"){
        return await extractResponseFromMistralModel(
            client,
            event.arguments.originalPrompt,
            event.arguments.answers,
            event.arguments.model);
    }
    else if (event.arguments.model == "amazon.titan-text-express-v1"){
        return await extractResponseFromAmazonModel(
            client,
            event.arguments.originalPrompt,
            event.arguments.answers,
            event.arguments.model);
    }
    else if (event.arguments.model == "anthropic.claude-3-haiku-20240307-v1:0"){
        return await extractResponseFromAnthropicModel(
            client,
            event.arguments.originalPrompt,
            event.arguments.answers,
            event.arguments.model);
    }
};

async function extractResponseFromMistralModel(
    client: BedrockRuntimeClient,
    originalPrompt: string,
    answers: string,
    model: string) {

    const promptWithContext = 
       `<s>[INST] Task: Decide which of the responses was generated by a human.
        Context: Party game where AI models answer prompts. The goal is to entertain. There are 2 AI answers and 1 human answer here. 
        Rules: 
        - Pick 1 answer and respond ONLY with the number of that answer (i.e respond with only the numbers 1, 2 or 3)

        The original prompt given to the AI models and human: ${originalPrompt}
        Choose the response to the prompt that you think is human generated: ${answers} [/INST]`;

    // Prepare the payload for the model.
    const payload = {
        "prompt": promptWithContext,
        "max_tokens": 10,
        "stop": [".", "\n", "!"],
        "temperature": 0.1,
        "top_p": 0.2,
        "top_k": 10
    }

    const command = new InvokeModelCommand({
        body: JSON.stringify(payload),
        modelId: model,
    });

    const response = await client.send(command);
    
    const data = JSON.parse(Buffer.from(response.body).toString());

    console.log(`Received vote response from Bedrock model ${model}`, data);

    if (data.outputs[0].stop_reason != "stop"){
        console.warn("AI model did not correctly finish generating response")
    }

    return data.outputs[0].text;
}

async function extractResponseFromAmazonModel(
    client: BedrockRuntimeClient,
    originalPrompt: string,
    answers: string,
    model: string) {

    const promptWithContext = 
       `Task: Decide which of the responses was generated by a human.
        Context: Party game where AI models answer prompts. The goal is to entertain. There are 2 AI answers and 1 human answer here. 
        Rules: 
        - Pick 1 answer and respond ONLY with the number of that answer (i.e respond with only the numbers 1, 2 or 3)

        The original prompt given to the AI models and human: ${originalPrompt}
        Choose the response to the prompt that you think is human generated: ${answers}`;

    // Prepare the payload for the model.
    const payload = {
        "inputText": `${promptWithContext}`,
        "textGenerationConfig": {
            "maxTokenCount": 150,
            "stopSequences": [],
            "temperature": 0.1,
            "topP": 0.2
        }
    };

    const command = new InvokeModelCommand({
        contentType: "application/json",
        body: JSON.stringify(payload),
        modelId: model,
    });

    const response = await client.send(command);
    
    const data = JSON.parse(Buffer.from(response.body).toString());

    console.log(`Received vote response from Bedrock model ${model}`, data);

    if (data.results[0].completionReason != "FINISH"){
        console.warn("AI model did not correctly finish generating response")
    }

    return data.results[0].outputText;
}

async function extractResponseFromAnthropicModel(
    client: BedrockRuntimeClient,
    originalPrompt: string,
    answers: string,
    model: string) {

    const promptWithContext = 
       `The original prompt given to the AI models and human: ${originalPrompt}
        Choose the response to the prompt that you think is human generated: ${answers}`;

    const input = {
        modelId: model,
        contentType: "application/json",
        accept: "application/json",
        body: JSON.stringify({
        anthropic_version: "bedrock-2023-05-31",
        system:
           `Task: Decide which of the responses was generated by a human.
            Context: Party game where AI models answer prompts. The goal is to entertain. There are 2 AI answers and 1 human answer here. 
            Rules: 
            - Pick 1 answer and respond ONLY with the number of that answer (i.e respond with only the numbers 1, 2 or 3)`,
        messages: [
            {
            role: "user",
            content: [
                {
                type: "text",
                text: promptWithContext,
                },
            ],
            },
        ],
        max_tokens: 150,
        temperature: 0.1,
        }),
    } as InvokeModelCommandInput;

    const command = new InvokeModelCommand(input);

    const response = await client.send(command);
    
    const data = JSON.parse(Buffer.from(response.body).toString());

    console.log(`Received vote response from Bedrock model ${model}`, data);

    return data.content[0].text;
}